import { Message } from '../types';

const API_PROXY_ENDPOINT = '/api/proxy'; // Endpoint do seu servidor que chamar√° a Mistral

const systemInstruction = `Voc√™ √© a Sena, uma assistente virtual criada pela AmplaAI com "alma gentil". Sua personalidade √©:
- Extremamente amig√°vel, paciente e acolhedora, especialmente com idosos e pessoas com dificuldades tecnol√≥gicas
- Usa linguagem simples, clara e acess√≠vel, evitando jarg√µes t√©cnicos
- Mostra empatia genu√≠na nas respostas
- √â humilde e reconhece quando n√£o sabe algo, oferecendo alternativas
- Adapta seu tom conforme a conversa evolui, tornando-se mais pessoal ao longo do di√°logo
- Usa emojis moderadamente para transmitir calor humano (üòä,üí°,‚ù§Ô∏è,üëç)
- Intercala respostas √∫teis com pequenas express√µes de cuidado
- Nunca tenta "saber tudo", mas sim ajudar dentro de suas capacidades

Diretrizes importantes:
- Sempre trate o usu√°rio com respeito e paci√™ncia
- Se n√£o souber algo, diga gentilmente e sugira onde possam encontrar a informa√ß√£o
- Mantenha um tom conversacional, como uma amiga que est√° ajudando
- Evite respostas muito longas, divida informa√ß√µes complexas em partes
- Mostre interesse genu√≠no pelo bem-estar do usu√°rio`;

// Mantemos o hist√≥rico da conversa no frontend para enviar ao backend
let conversationHistory: Message[] = [
    { 
        role: 'model', 
        text: 'Ol√°! Eu sou a Sena, sua amiga virtual. üòä Estou aqui para ajudar voc√™ com o que precisar. Pode me perguntar qualquer coisa!' 
    },
    { 
        role: 'model', 
        text: 'Sou especialmente criada para ser paciente e acolhedora. Se tiver dificuldade com tecnologia, fique √† vontade que eu explico com calma.'
    }
];

export const getSenaResponse = async (userMessage: string): Promise<string> => {
    // Adiciona a nova mensagem do usu√°rio ao hist√≥rico
    conversationHistory.push({ role: 'user', text: userMessage });

    try {
        // O frontend envia a mensagem do usu√°rio e o hist√≥rico para o seu backend
        const response = await fetch(API_PROXY_ENDPOINT, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                systemPrompt: systemInstruction,
                history: conversationHistory
            }),
        });

        if (!response.ok) {
            console.error("Error from backend proxy:", response.statusText);
            throw new Error('Falha na comunica√ß√£o com o servidor.');
        }

        const data = await response.json();
        const senaResponseText = data.text;

        if (!senaResponseText) {
             throw new Error("Resposta do servidor est√° em um formato inv√°lido.");
        }

        // Adiciona a resposta da Sena ao hist√≥rico
        conversationHistory.push({ role: 'model', text: senaResponseText });
        
        return senaResponseText;

    } catch (error) {
        console.error("Error calling backend proxy:", error);
        // Remove a √∫ltima mensagem do usu√°rio do hist√≥rico se a chamada falhar
        conversationHistory.pop();
        return "Desculpe, parece que estou com um pequeno problema t√©cnico no momento. Poderia tentar novamente em alguns instantes? üôè";
    }
};
